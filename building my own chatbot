import numpy as np
import nltk
import string
import random



f = open('C://Users//Rahul//OneDrive//Desktop//data science 101//python file//chatbot.txt','r', errors='ignore')
raw_doc = f.read()
raw_doc = raw_doc.lower()
# nltk.download('punkt')
# nltk.download('wordnet')
sent_tokens = nltk.sent_tokenize(raw_doc)
word_sent = nltk.word_tokenize(raw_doc)

print(sent_tokens[:2])
print(word_sent[:2])


### for data preprocessing #########

lemmer = nltk.stem.WordNetLemmatizer()

def LemTokens(tokens):
    return[lemmer.lemmatize(token)for token in tokens]
remove_punct_dict = dict((ord(punct),None)for punct in string.punctuation)

def LemNormalize(text):
    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

Greet_inputs = ("Hello", "hi", "Greetings","sup","what's Up!!", "What's Up Hommie","Hey","What's Up Dog!!")
Greet_Response = ["Hi","Hello","Whats Up Hommie","Whats nigga","whats up Sport","I am Glad, that you are talking to me"]

## defining the greet function #######

def greet(sentence):
    for word in sentence.split():
        if word.lower() in Greet_inputs:
            return random.choice(Greet_Response)

### Response Generation ####

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def response(user_response):
    robo1_response = ''
    TfidfVec = TfidfVectorizer(tokensizer = LemNormalize,stop_words = 'english')
    tfidf = cosine_similarity(tfidf[-1],tfidf)
    idx = vals.argsort()[0][-2]
    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]
    if(req_tfidf==0):
        robo1_response = robo1_response+ " Sorry man! I cant help you with that"
        return robo1_response
    else:
        robo1_response  = robo1_response+ sent_tokens[idx]
        return robo1_response    
